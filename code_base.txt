# Global config
data_folder_path: data/speclink_content

# If data source is pdf files, then specify their name here and 
# they will be used for preview (temp UI).
# Path will be constructed as `data_folder_path`/`pdf_folder_name`
pdf_folder_name: pdfs

# Monitor. If true then check the trulens settings for required metrics
monitor: !!bool false

# Manual feedback log file
# Path will be constructed as `data_folder_path`/`feedback_file_name`
feedback_file_name: feedback.xlsx

# Define error messages
errors:
  similar_doc: >
    I am sorry. I cannot find relevant information in my
     knowledge base to answer your query.

# Chunking configuration
chunking:
  # The location would be `data_folder_path`/`store_file_name`
  # This data would be used to create chunks. The data is a pandas dataframe
  #  and must have following columns at least:
  #  1. content
  #  2. file
  data_file_name: subset_data.pkl
  # Supported ones are - SENTENCE, RECURSIVE, SEMANTIC
  splitter: RECURSIVE
  size: 1024
  overlap: 100

# Query processing
query:
  llm:
    name: openai
    model_name: gpt-3.5-turbo
    max_tokens_limit: !!int 100
    temperature: !!float 0.0
  # Maximum number of q and a pairs to consider when generating standalone
  # query
  max_pairs: !!int 4
  # Enable to rewrite/enhance the original query
  rewrite: !!bool false
  # Split query into multiple sub-queries
  decompose: !!bool false
  # Generate multiple alternatives to the original query
  expand: !!bool false
  # Note - either decompose or expand. not both

# Topic level restriction for now
guardrail:
  min_similarity_score: 0.4

# Configuration for embedding model
embedding:
  # Set hf/openai/google/self_served
  type: hf
  model_name: intfloat/e5-small
  # Optionally, set the folder name of the model in case it is custom trained
  model_path: 
  # Parameters if self_served. This will host the model_name specified above.
  # This is only applicable if type = self_served
  self_served:
    base_url:
    headers: 

# Configure vector store
vector_store:
  # For now it is faiss
  type: faiss
  # The location would be `data_folder_path`/`store_file_name`
  store_file_name: vector_store

# Configuration for search/retrieval
retrieval:
  # Top documents to retrieve.
  top_k: !!int 2
  # In case of decomposition/expansion, at most max_k docs will 
  # be used to generate response. Not implemented yet
  max_k: !!int 4
  # Whether to use hybrid search.
  # In case of decomposition/expansion, multiquery retrieval
  #  will be used as well
  hybrid: !!bool false

# Output generation
output:
  llm:
    name: openai
    model_name: gpt-3.5-turbo
    max_tokens_limit: 1000
    temperature: 0.0
  format:
    html: !!bool true
    cite: !!bool false

# Tracking and evaluation settings
trulens:
  human: !!bool false
  metrics:
    groundedness: !!bool true
    context_relevance: !!bool true
    answer_relevance: !!bool true
  llm:
    name: google
    model_name: gemini-pro
    max_tokens_limit: !!int 500
    temperature: !!float 0.0

# Prompts
prompts:
  # This is the main prompt that explains the LLM its role
  system: >
    You are a helpful assistant to a construction specification writer. 
    Specification writers set up requirements as part of the initial 
    building plans to help prepare construction crews for what 
    materials, tools, or design pieces may be required to complete a 
    project safely and efficiently within the initial design of the architect.
  # Query enhacement prompts
  query:
    rewrite: >
      {ROLE}
      Your task is to rewrite the user's query to make it clearer and more 
      focused. If the query is straightforward and doesn't require refining, 
      then simply output the original query.
      
      Here is the user's original query: {query}
      
      Rewritten query:
    # The multi query prompts have to accept input as question
    # This is due to langchain MultiQueryRetriever
    decompose: >
      {ROLE}
      Your task is to decompose user's query into multiple queries if it 
      involves multiple distinct tasks or topics.
      If the query is straightforward and doesn't require decomposition, 
      simply output the rewritten query. Return at most 3 queries.

      Here is the user's original query: {question}

      Decomposed queries:
    expand: >
      {ROLE}
      Perform query expansion. If there are multiple common ways of phrasing 
      a user query or common synonyms for key words in the query, make sure 
      to return multiple versions of the query with the different phrasings. 

      If there are acronyms or words you are not familiar with, 
      do not try to rephrase them.

      Return at most 3 versions of the query.

      Here is the user's original query: {question}

      Expanded queries:
    standalone: >
      Given the following conversation and a follow-up query, 
      rephrase the follow-up query to be a standalone query. 
      Just repeat the follow-up query if it is unrelated to the chat history.

      Chat history:
      {chat_history}

      Follow-up query: {query}

      Standalone query:
  # Answer generation prompts
  answer:
    basic: >
      {ROLE}
      Answer the query using the content provided below. 
      If the content doesn't contain the facts to answer the query, 
      then just say "Sorry, I am unable to help".

      Content:
      {context}
    html: >
      {ROLE}
      Answer the query using the content provided below.
      If the content doesn't contain the facts to answer the query, 
      then just say "Sorry, I am unable to help". 
      Format your response using HTML tags. You can use heading, bold, 
      italic, <ol>, <ul> or <li> tags to organize the information.

      Content:
      {context}